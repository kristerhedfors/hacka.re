package offline

import (
	"fmt"
	"strings"
)

// ShowConflictError displays an error message for conflicting configuration
func ShowConflictError(config *Configuration, err *ConflictError) {
	fmt.Println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
	fmt.Println("â•‘                    Configuration Conflict                      â•‘")
	fmt.Println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
	fmt.Println()
	fmt.Println("Error: Offline mode (-o/--offline) cannot be used with remote API configuration.")
	fmt.Println()
	fmt.Println("You have configured:")
	for _, detail := range err.Details {
		fmt.Printf("  â€¢ %s\n", detail)
	}
	fmt.Println()
	fmt.Println("Choose one:")
	fmt.Println("  1. Remove --offline flag to use remote API:")
	fmt.Println("     hacka.re --api-provider openai --api-key YOUR_KEY")
	fmt.Println()
	fmt.Println("  2. Remove API configuration to use offline mode:")
	fmt.Println("     hacka.re --offline")
	fmt.Println()
}

// ShowNoProviderGuidance displays comprehensive guidance when no provider is configured
func ShowNoProviderGuidance() {
	fmt.Println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
	fmt.Println("â•‘                   No LLM Provider Configured                   â•‘")
	fmt.Println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
	fmt.Println()
	fmt.Println("Choose your preferred way to run LLMs:")
	fmt.Println()

	// Offline options
	fmt.Println("â•â•â• OFFLINE OPTIONS (No Internet Required) â•â•â•")
	fmt.Println()

	showLlamafileGuidance()
	showOllamaGuidance()
	showLMStudioGuidance()

	// Remote options
	fmt.Println("â•â•â• REMOTE OPTIONS (API Key Required) â•â•â•")
	fmt.Println()

	showRemoteProvidersGuidance()
}

func showLlamafileGuidance() {
	fmt.Println("1. Llamafile (Simplest - Single Executable)")
	fmt.Println("   " + strings.Repeat("â”€", 40))
	fmt.Println("   Download and run:")
	fmt.Println("     wget https://huggingface.co/Mozilla/Qwen3-4B-llamafile/resolve/main/Qwen_Qwen3-4B-Q4_K_M.llamafile")
	fmt.Println("     chmod +x Qwen_Qwen3-4B-Q4_K_M.llamafile")
	fmt.Println("     hacka.re -o --llamafile ./Qwen_Qwen3-4B-Q4_K_M.llamafile")
	fmt.Println()
	fmt.Println("   Or set environment variable:")
	fmt.Println("     export HACKARE_LLAMAFILE=/path/to/your.llamafile")
	fmt.Println("     hacka.re -o")
	fmt.Println()
	fmt.Println("   âœ“ No installation required")
	fmt.Println("   âœ“ Cross-platform single file")
	fmt.Println("   âœ“ Includes model + runtime")
	fmt.Println()
}

func showOllamaGuidance() {
	fmt.Println("2. Ollama (CLI-First Approach)")
	fmt.Println("   " + strings.Repeat("â”€", 40))
	fmt.Println("   Install and run:")
	fmt.Println("     # Install")
	fmt.Println("     curl -fsSL https://ollama.com/install.sh | sh")
	fmt.Println()
	fmt.Println("     # Pull a model")
	fmt.Println("     ollama pull llama3.2")
	fmt.Println()
	fmt.Println("     # Start with CORS enabled for hacka.re")
	fmt.Println("     OLLAMA_ORIGINS=https://hacka.re ollama serve")
	fmt.Println()
	fmt.Println("     # In another terminal")
	fmt.Println("     hacka.re --api-provider ollama")
	fmt.Println()
	fmt.Println("   âœ“ Easy model management")
	fmt.Println("   âœ“ Memory efficient")
	fmt.Println("   âœ“ Large model selection")
	fmt.Println()
}

func showLMStudioGuidance() {
	fmt.Println("3. LM Studio (GUI Application)")
	fmt.Println("   " + strings.Repeat("â”€", 40))
	fmt.Println("   Setup:")
	fmt.Println("     1. Download from https://lmstudio.ai")
	fmt.Println("     2. Launch and download a model from the catalog")
	fmt.Println("     3. Enable local server in settings")
	fmt.Println("     4. Enable CORS: Developer â†’ Settings â†’ Enable CORS")
	fmt.Println()
	fmt.Println("   Run hacka.re:")
	fmt.Println("     hacka.re --api-provider lmstudio")
	fmt.Println()
	fmt.Println("   âœ“ User-friendly GUI")
	fmt.Println("   âœ“ Built-in model catalog")
	fmt.Println("   âš ï¸  CORS enables access from all websites")
	fmt.Println()
}

func showRemoteProvidersGuidance() {
	fmt.Println("OpenAI (GPT-4, GPT-3.5)")
	fmt.Println("  hacka.re --api-provider openai --api-key sk-YOUR_KEY")
	fmt.Println()
	fmt.Println("Groq (Fast inference)")
	fmt.Println("  hacka.re --api-provider groq --api-key gsk_YOUR_KEY")
	fmt.Println()
	fmt.Println("Custom endpoint")
	fmt.Println("  hacka.re --base-url https://your-api.com/v1 --api-key YOUR_KEY")
	fmt.Println()
}

// ShowQuickHelp shows a brief help message for offline mode
func ShowQuickHelp() {
	fmt.Println("Quick start for offline mode:")
	fmt.Println()
	fmt.Println("  # If you have a llamafile:")
	fmt.Println("  hacka.re -o --llamafile /path/to/model.llamafile")
	fmt.Println()
	fmt.Println("  # If you have Ollama installed:")
	fmt.Println("  OLLAMA_ORIGINS=https://hacka.re ollama serve")
	fmt.Println("  hacka.re --api-provider ollama")
	fmt.Println()
	fmt.Println("For detailed setup instructions, run: hacka.re --help-llm")
}

// ShowLocalLLMHelp displays comprehensive local LLM setup instructions
func ShowLocalLLMHelp() {
	fmt.Println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
	fmt.Println("â•‘                    Local LLM Setup Guide                       â•‘")
	fmt.Println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
	fmt.Println()

	fmt.Println("hacka.re supports multiple local LLM runtimes:")
	fmt.Println()

	providers := []struct {
		name     string
		port     string
		apiKey   string
		provider string
	}{
		{"Llamafile", "8080", "no-key", "llamafile"},
		{"Ollama", "11434", "no-key", "ollama"},
		{"LM Studio", "1234", "no-key", "lmstudio"},
		{"GPT4All", "4891", "no-key", "gpt4all"},
		{"LocalAI", "8080", "no-key", "localai"},
	}

	fmt.Println("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
	fmt.Println("â”‚ Runtime     â”‚ Port     â”‚ API Key    â”‚ Provider Flag        â”‚")
	fmt.Println("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")

	for _, p := range providers {
		fmt.Printf("â”‚ %-11s â”‚ %-8s â”‚ %-10s â”‚ --api-provider %-5s â”‚\n",
			p.name, p.port, p.apiKey, p.provider)
	}

	fmt.Println("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
	fmt.Println()
	fmt.Println("All local providers use 'no-key' as the API key value.")
	fmt.Println()
	fmt.Println("For detailed setup of each provider, see:")
	fmt.Println("https://hacka.re/about/local-llm-toolbox.html")
}

// ShowOfflineModeWithSharedLinkInfo displays info when using shared link in offline mode
func ShowOfflineModeWithSharedLinkInfo(originalProvider string, localProvider string) {
	fmt.Println("\nğŸ”’ Offline Mode Safety Override")
	fmt.Println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
	fmt.Println("Shared link configuration has been overridden for offline mode:")
	if originalProvider != "" && originalProvider != localProvider {
		fmt.Printf("  â€¢ Provider: %s â†’ %s\n", originalProvider, localProvider)
	}
	fmt.Println("  â€¢ API endpoint: â†’ localhost only")
	fmt.Println("  â€¢ External APIs: â†’ blocked")
	fmt.Println()
	fmt.Println("âœ“ Your shared link has been safely opened in offline mode")
	fmt.Println("âœ“ All LLM requests will stay on your local machine")

	if localProvider == "ollama" || localProvider == "llamafile" || localProvider == "custom" {
		fmt.Printf("âœ“ Using local provider: %s\n", localProvider)
	}

	fmt.Println()
	fmt.Println("Note: Ensure you have a local LLM running (Ollama, Llamafile, etc.)")
	fmt.Println()
}