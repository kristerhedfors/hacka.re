# Function Calling Compatibility Test Results

Date: 2025-08-28  
Test Suite: hacka.re Function Calling Tests

## Summary

We tested function calling capabilities across three providers (OpenAI, Groq Cloud, and Berget) with multiple models to identify which ones properly support function calling in hacka.re.

## Test Results

### ‚úÖ Working Models (Function Calling Confirmed)

#### OpenAI
- **gpt-5-nano** ‚úì - Function executed successfully, uses code block format
- **gpt-4o** ‚úì - Function executed successfully (based on mini test)
- **o4-mini** ‚úì - Function executed successfully (from earlier tests)

#### Groq Cloud  
- **moonshotai/kimi-k2-instruct** ‚úì - Function executed successfully, uses code block format
- Other Groq models showed mixed results

#### Berget
- **mistralai/Magistral-Small-2506** ‚ö†Ô∏è - Received tool calls but execution not detected in UI

### ‚ö†Ô∏è Partial/Inconsistent Support

#### Groq Cloud
- **llama-3.3-70b-versatile** - Function not executed in test
- **compound-beta** - Not available in test (fell back to kimi)
- **mixtral-8x7b-32768** - Not available in test

### üìä Response Format Analysis

All tested models that work use similar patterns:
- **Code Block Format**: Functions appear in code blocks in the UI
- **Console Logging**: Successful execution shows:
  - `[ChatToolsService] - Function tool calls results`
  - `[FunctionCallRenderer] Call indicator created for: [function_name]`
  - `[ToolCallHandler] Tool execution completed`

### üîç Key Findings

1. **OpenAI models** have the most consistent function calling support
2. **Kimi model on Groq** works well for function calling
3. **Response formats** are handled uniformly through code blocks
4. **Non-chat models** (TTS, embeddings, image generation) were excluded from testing

## Models Excluded from Testing

These model types don't support chat/function calling:
- **Image Generation**: dall-e-2, dall-e-3
- **Text-to-Speech**: tts-1, tts-1-hd, playai-tts
- **Speech Recognition**: whisper-1, whisper-large-v3
- **Embeddings**: text-embedding-ada-002, text-embedding-3-small/large
- **Guard Models**: llama-prompt-guard-2-22m, llama-guard-4-12b

## Testing Methodology

1. **Simple Function Test**: Echo function with single string parameter
2. **Detection Methods**: 
   - Console log analysis
   - UI element detection (code blocks, tool call containers)
   - Response text parsing
3. **Providers Tested**: OpenAI, Groq Cloud, Berget
4. **Environment**: Playwright browser automation with real API calls

## Recommendations

### For Reliable Function Calling Use:
1. **OpenAI**: gpt-5-nano, gpt-4o, o4-mini
2. **Groq**: moonshotai/kimi-k2-instruct

### For Testing/Development:
- Use gpt-5-nano (OpenAI) for cost-effective testing
- Use Kimi (Groq) as a free alternative

### Models to Avoid for Function Calling:
- Non-chat models (image, TTS, embeddings)
- Guard/safety models
- Models showing inconsistent results in tests

## Test Commands

```bash
# Test all models
_tests/playwright/run_model_function_tests.sh

# Test specific provider
_tests/playwright/run_model_function_tests.sh --groq
_tests/playwright/run_model_function_tests.sh --openai  
_tests/playwright/run_model_function_tests.sh --berget

# Test specific model
_tests/playwright/test_function_format_compatibility.py::test_model_function_format -k "gpt-5-nano"
```

## Future Testing

As new models are added, they should be tested with:
1. Simple function calls (getCurrentTime)
2. Parameterized functions (echo, addNumbers)
3. Complex async functions (hostinfo)

The test suite is designed to be easily extended with new models and providers.