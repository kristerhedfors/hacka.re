<!DOCTYPE html>
<html lang="en" class="theme-modern">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>hacka.re - Local LLM Toolbox</title>
    <meta name="description" content="Simple guide to running LLMs locally with hacka.re">
    <link rel="stylesheet" href="../css/styles.css">
    <link rel="stylesheet" href="css/themes.css">
    <!-- Font Awesome for icons (local) -->
    <link rel="stylesheet" href="../lib/font-awesome/all.min.css">
    <!-- Markdown parser (local) -->
    <script src="../lib/marked/marked.min.js"></script>
    <script src="../lib/dompurify/purify.min.js"></script>
    <style>
        .toolbox-header {
            background-color: var(--ai-msg-bg);
            border-radius: var(--border-radius);
            padding: 20px;
            margin: 20px 0;
            border-left: 4px solid var(--accent-color);
            font-family: 'Courier New', monospace;
        }

        .runtime-section {
            margin: 40px 0;
            border-top: 2px solid var(--border-color);
            padding-top: 30px;
        }

        .runtime-section h2 {
            color: var(--primary-color);
            margin-bottom: 20px;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .runtime-section h3 {
            margin-top: 25px;
            margin-bottom: 15px;
        }

        .quick-start {
            background-color: var(--system-msg-bg);
            border-radius: var(--border-radius);
            padding: 15px;
            margin: 20px 0;
        }

        .code-block {
            background-color: rgba(0, 0, 0, 0.05);
            border-radius: var(--border-radius);
            padding: 15px;
            margin: 15px 0;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            position: relative;
        }
        
        .code-block pre {
            margin: 0;
            white-space: pre;
            word-wrap: normal;
            overflow-x: auto;
        }
        
        .copy-button {
            position: absolute;
            top: 8px;
            right: 8px;
            background-color: var(--primary-color);
            color: white;
            border: none;
            border-radius: 4px;
            padding: 4px 8px;
            font-size: 12px;
            cursor: pointer;
            opacity: 0.7;
            transition: opacity 0.3s;
        }
        
        .copy-button:hover {
            opacity: 1;
        }
        
        .copy-button:active {
            background-color: var(--primary-dark);
        }

        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .pros, .cons {
            background-color: var(--ai-msg-bg);
            border-radius: var(--border-radius);
            padding: 15px;
        }

        .pros h4, .cons h4 {
            color: var(--primary-color);
            margin-bottom: 10px;
        }

        .decision-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        .decision-table th, .decision-table td {
            padding: 12px;
            border: 1px solid var(--border-color);
            text-align: left;
        }

        .decision-table th {
            background-color: var(--primary-color);
            color: white;
        }

        .decision-table tr:nth-child(even) {
            background-color: var(--ai-msg-bg);
        }

        .page-nav {
            display: flex;
            justify-content: space-between;
            margin: 20px 0;
            flex-wrap: wrap;
            gap: 10px;
        }

        .page-nav a {
            padding: 8px 15px;
            background-color: var(--primary-color);
            color: white;
            border-radius: var(--border-radius);
            text-decoration: none;
            transition: background-color 0.3s;
        }

        .page-nav a:hover {
            background-color: var(--primary-dark);
            text-decoration: none;
        }

        ul li {
            margin-bottom: 8px;
        }

        @media (max-width: 768px) {
            .pros-cons {
                grid-template-columns: 1fr;
            }

            .decision-table {
                font-size: 0.9em;
            }
        }
    </style>
</head>
<body>
    <div class="app-container" style="max-width: none; width: 100%;">
        <div class="beta-tag">BETA</div>
        <header>
            <div class="logo">
                <h1 style="line-height: 1.2;"><a href="../index.html" style="text-decoration: none; color: inherit; font-family: 'Courier New', monospace;">hacka.re</a><span style="font-size: 70%;">: serverless agency</span><br><span style="font-size: 60%; font-weight: normal;">Free, open, för hackare, av hackare.</span></h1>
            </div>
            <div class="settings">
                <a href="../index.html" class="btn secondary-btn" style="margin-right: 10px; color: #ffffff; background-color: rgba(0, 0, 0, 0.2); border: 1px solid #ffffff;">Back to Chat</a>
            </div>
        </header>

        <main style="overflow-y: auto; overflow-x: auto; padding: 2rem;">
            <div class="container" style="max-width: 1000px; margin: 0 auto;">
            
            <h1>Local LLM Toolbox</h1>
            
            <div class="page-nav">
                <a href="index.html">About</a>
                <a href="architecture.html">Architecture</a>
                <a href="development.html">Development</a>
                <a href="local-llm-toolbox.html">Local LLMs</a>
                <a href="research-report.html">Research Report</a>
                <a href="disclaimer.html">Disclaimer</a>
            </div>

            <div class="toolbox-header">
                <h3>HACKA.RE - LOCAL LLM TOOLBOX</h3>
                <p>Simple guide to running Large Language Models on your own hardware</p>
            </div>

            <div class="runtime-section">
                <h2>Context</h2>
                <p>
                    As of June 2025, the local LLM ecosystem has matured significantly. Five major projects dominate the landscape for running models on personal hardware without cloud dependencies:
                </p>
                <ul>
                    <li><strong>Ollama</strong> (Ollama Inc.) — The CLI-first approach</li>
                    <li><strong>Llamafile</strong> (Mozilla) — Single-executable simplicity</li>
                    <li><strong>GPT4All</strong> (Nomic AI) — Desktop app + CLI</li>
                    <li><strong>LM Studio</strong> (Element Labs) — GUI-first with SDK</li>
                    <li><strong>LocalAI</strong> (Community) — Go-based OpenAI shim</li>
                </ul>
                <p>
                    All five expose OpenAI-compatible HTTP interfaces and prioritize privacy through offline operation, making them perfect companions for hacka.re's serverless architecture.
                </p>
            </div>

            <div class="runtime-section">
                <h2>1. Ollama — The Developer's Choice</h2>
                
                <div class="quick-start">
                    <h3>Quick-start</h3>
                    <div class="code-block">
                        <button class="copy-button" onclick="copyToClipboard(this)" title="Copy to clipboard">Copy</button>
                        <pre># macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh
ollama serve

# Windows (PowerShell as admin)
Invoke-WebRequest -Uri https://ollama.com/download/OllamaSetup.exe -OutFile OllamaSetup.exe
.\OllamaSetup.exe

# Pull and run a model
ollama pull llama3.3:7b
ollama run llama3.3:7b</pre>
                    </div>
                    <p>Point hacka.re to <code>http://localhost:11434/v1/chat/completions</code> (no API key required)</p>
                </div>

                <div class="pros-cons">
                    <div class="pros">
                        <h4>Pros</h4>
                        <ul>
                            <li><strong>CLI-native</strong>: Perfect for developers and automation</li>
                            <li><strong>Model library</strong>: Curated collection with one-line pulls</li>
                            <li><strong>Memory efficient</strong>: Automatic GPU/CPU offloading</li>
                            <li><strong>Docker-friendly</strong>: Official images for containerized deployment</li>
                            <li><strong>Active development</strong>: Frequent updates, strong community</li>
                        </ul>
                    </div>
                    <div class="cons">
                        <h4>Cons</h4>
                        <ul>
                            <li>No built-in GUI (terminal only)</li>
                            <li>macOS installation requires admin privileges</li>
                            <li>Model format locked to Ollama's structure</li>
                            <li>Limited Windows GPU support (CUDA only)</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="runtime-section">
                <h2>2. Llamafile — Zero-Dependency Portability</h2>
                
                <div class="quick-start">
                    <h3>Quick-start</h3>
                    <div class="code-block">
                        <button class="copy-button" onclick="copyToClipboard(this)" title="Copy to clipboard">Copy</button>
                        <pre>wget https://huggingface.co/Mozilla/Qwen3-4B-llamafile/resolve/main/Qwen_Qwen3-4B-Q4_K_M.llamafile
chmod +x Qwen_Qwen3-4B-Q4_K_M.llamafile
./Qwen_Qwen3-4B-Q4_K_M.llamafile</pre>
                    </div>
                    <p>Point hacka.re to <code>http://localhost:8080/v1/chat/completions</code> (no API key required)</p>
                </div>

                <div class="pros-cons">
                    <div class="pros">
                        <h4>Pros</h4>
                        <ul>
                            <li><strong>True portability</strong>: Single file includes model + runtime</li>
                            <li><strong>No installation</strong>: Download and run, that's it</li>
                            <li><strong>Cross-platform binary</strong>: Same file works everywhere</li>
                            <li><strong>Mozilla backing</strong>: Quality engineering, security focus</li>
                            <li><strong>Cosmopolitan libc</strong>: Innovative polyglot executables</li>
                        </ul>
                    </div>
                    <div class="cons">
                        <h4>Cons</h4>
                        <ul>
                            <li>Large file sizes (model bundled with runtime)</li>
                            <li>One model per executable (no model switching)</li>
                            <li>Limited model selection vs. other platforms</li>
                            <li>macOS Gatekeeper warnings on first run</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="runtime-section">
                <h2>3. GPT4All — Desktop App + CLI</h2>
                
                <div class="quick-start">
                    <h3>Quick-start (identical on Win/macOS/Linux)</h3>
                    <ol>
                        <li>Download installer from <a href="https://gpt4all.io" target="_blank">gpt4all.io</a>
                            <ul>
                                <li>Windows: run the MSI</li>
                                <li>macOS: drag to /Applications</li>
                                <li>Linux: untar + chmod +x</li>
                            </ul>
                        </li>
                        <li>Launch GUI once (creates <code>~/.gpt4all</code>, starts server on <code>localhost:4891</code>)</li>
                        <li>Point hacka.re to <code>http://localhost:4891/v1/chat/completions</code> (no key expected)</li>
                    </ol>
                </div>

                <div class="pros-cons">
                    <div class="pros">
                        <h4>Pros</h4>
                        <ul>
                            <li>One installer, cross-platform; no Python/Docker needed</li>
                            <li>Ships dozens of GGUF builds; model picker built in</li>
                            <li><strong>Offline-by-design</strong>: no telemetry; CPU-only hardware OK</li>
                            <li>Integrated chat UI for non-technical users</li>
                        </ul>
                    </div>
                    <div class="cons">
                        <h4>Cons</h4>
                        <ul>
                            <li>GUI mandatory for first run; headless requires CLI flags</li>
                            <li>Closed-source Electron shell (core libs MIT, GUI proprietary)</li>
                            <li>Non-standard API port (4891) requires config in hacka.re</li>
                            <li>Memory hungry with GUI running</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="runtime-section">
                <h2>4. LM Studio — GUI, CLI (lms), and SDK</h2>
                
                <div class="quick-start">
                    <h3>Quick-start</h3>
                    <ul>
                        <li>Windows/macOS: download <em>LM Studio</em> app; Linux: AppImage or tar</li>
                        <li>First launch opens model catalogue; pick a GGUF, click <strong>Run</strong></li>
                        <li>Enable "Local LLM Server" in settings → serves on <code>localhost:1234</code></li>
                    </ul>
                </div>

                <div class="pros-cons">
                    <div class="pros">
                        <h4>Pros</h4>
                        <ul>
                            <li>Polished catalogue and log viewer; zero terminal steps</li>
                            <li>Promise of <strong>no data collection; everything local</strong></li>
                            <li>MIT-licensed CLI (<code>lms</code>) and JS/Python SDK</li>
                            <li>Advanced model configuration UI</li>
                        </ul>
                    </div>
                    <div class="cons">
                        <h4>Cons</h4>
                        <ul>
                            <li>GUI binaries closed-source; only SDK & CLI are MIT</li>
                            <li>Fixed port 1234; no concurrent model hosting</li>
                            <li>Requires AVX2; older CPUs (pre-2013) unsupported</li>
                            <li>Large download size (~500MB)</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="runtime-section">
                <h2>5. LocalAI — Go Service as OpenAI Drop-in</h2>
                
                <div class="quick-start">
                    <h3>Quick-start (any OS with Go ≥1.22 or via Docker/Podman)</h3>
                    <div class="code-block">
                        <button class="copy-button" onclick="copyToClipboard(this)" title="Copy to clipboard">Copy</button>
                        <pre>curl -sSL https://localai.io/install.sh | bash
localai serve

# Download a model definition (yaml) into /models, then:
localai pull stablelm-3b-4q
localai run stablelm-3b-4q</pre>
                    </div>
                </div>

                <div class="pros-cons">
                    <div class="pros">
                        <h4>Pros</h4>
                        <ul>
                            <li>Pure binary (static Go + llama.cpp); single file ≈ 60 MB</li>
                            <li><strong>OpenAI shim</strong> lets hacka.re talk with <em>zero</em> code changes</li>
                            <li>MIT licence; CPU default, CUDA/ROCm builds available</li>
                            <li>Kubernetes charts for production deployment</li>
                        </ul>
                    </div>
                    <div class="cons">
                        <h4>Cons</h4>
                        <ul>
                            <li>Sparse GUI; manage YAML <em>and</em> GGUF files manually</li>
                            <li>Memory footprint grows with concurrent models</li>
                            <li>Release cadence can lag llama.cpp upstream</li>
                            <li>Configuration complexity for advanced features</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="runtime-section">
                <h2>How to Decide?</h2>
                
                <h3>Platform Defaults for hacka.re</h3>
                <table class="decision-table">
                    <thead>
                        <tr>
                            <th>OS</th>
                            <th>Primary</th>
                            <th>Fallback</th>
                            <th>Notes</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Windows</strong></td>
                            <td>GPT4All</td>
                            <td>LM Studio</td>
                            <td>GPT4All installer avoids WSL; LM Studio for GUI preference</td>
                        </tr>
                        <tr>
                            <td><strong>macOS</strong></td>
                            <td>Ollama</td>
                            <td>LM Studio</td>
                            <td>Ollama integrates perfectly; LM Studio for non-developers</td>
                        </tr>
                        <tr>
                            <td><strong>Linux</strong></td>
                            <td>Ollama</td>
                            <td>LocalAI</td>
                            <td>Both headless-friendly; choose per admin preference</td>
                        </tr>
                    </tbody>
                </table>

                <h3>Decision Criteria</h3>
                <ol>
                    <li><strong>User friction</strong> (GUI vs CLI)</li>
                    <li><strong>API surface</strong> (port, auth quirks)</li>
                    <li><strong>License clarity</strong> (all must remain FOSS or permissive)</li>
                    <li><strong>Update velocity & model catalogue</strong></li>
                </ol>
            </div>

            <div class="runtime-section">
                <h2>True Serverless: Complete Privacy with Local LLMs</h2>
                
                <p>
                    The combination of hacka.re's client-side architecture with local LLM runtimes achieves <strong>true serverless operation</strong> — eliminating the final privacy hurdle:
                </p>
                
                <div class="highlight-box" style="background-color: var(--system-msg-bg); border-radius: var(--border-radius); padding: 20px; margin: 20px 0; border-left: 4px solid var(--primary-color);">
                    <h3>🔐 Zero Data Leakage Architecture</h3>
                    <p>When using local LLMs with hacka.re:</p>
                    <ul>
                        <li><strong>No cloud API calls</strong> — LLM runs on your hardware</li>
                        <li><strong>No hacka.re servers</strong> — Just static files from CDN</li>
                        <li><strong>No telemetry</strong> — Neither hacka.re nor the LLM phone home</li>
                        <li><strong>Complete air-gap capable</strong> — Download hacka.re + LLM, disconnect internet</li>
                    </ul>
                </div>

                <h3>Serverless Flow with Local LLMs</h3>
                <div class="code-block"><pre>┌─────────────┐     ┌────────────────┐     ┌──────────────┐
│   Browser   │────▶│  Static Files  │────▶│  CDN/Local   │
│  (hacka.re) │◀────│   (index.html) │◀────│   Filesystem │
└──────┬──────┘     └────────────────┘     └──────────────┘
       │                                           
       │ localhost API calls                       
       │ (never leaves machine)                    
       ▼                                           
┌─────────────┐                                    
│ Local LLM   │                                    
│  (Ollama,   │                                    
│  llamafile, │                                    
│  etc.)      │                                    
└─────────────┘</pre></div>

            </div>

            <div class="runtime-section">
                <h2>Implementation Hooks for hacka.re</h2>
                
                <div class="code-block"><pre>// js/services/api-service.js — extend runtime selector
export const BACKENDS = {
  "ollama"    : { baseURL: "http://localhost:11434/v1" },
  "llamafile" : { baseURL: "http://localhost:8080/v1",
                  headers : { Authorization: "Bearer no-key" } },
  "gpt4all"   : { baseURL: "http://localhost:4891/v1" },
  "lmstudio"  : { baseURL: "http://localhost:1234/v1" },
  "localai"   : { baseURL: "http://localhost:8080/v1" }
};</pre></div>

                <h3>Quick Configuration in hacka.re</h3>
                <ol>
                    <li>Open Settings (gear icon)</li>
                    <li>Select "Custom" as API Provider</li>
                    <li>Enter the appropriate localhost URL from above</li>
                    <li>Leave API Key field empty (or enter "no-key" for llamafile)</li>
                    <li>Select your loaded model from the dropdown</li>
                </ol>
            </div>

            <div class="page-nav">
                <a href="index.html">About</a>
                <a href="architecture.html">Architecture</a>
                <a href="development.html">Development</a>
                <a href="local-llm-toolbox.html">Local LLMs</a>
                <a href="research-report.html">Research Report</a>
                <a href="disclaimer.html">Disclaimer</a>
            </div>

            </div>
        </main>

        <footer>
            <p><a href="https://hacka.re/" target="_blank" style="font-family: 'Courier New', monospace;">hacka.re</a> | <a href="../index.html">Back to Chat</a> | <a href="disclaimer.html">Disclaimer</a></p>
        </footer>
    </div>

    <script src="js/themes.js"></script>
    <script>
        function copyToClipboard(button) {
            const codeBlock = button.nextElementSibling;
            const text = codeBlock.textContent;
            
            navigator.clipboard.writeText(text).then(function() {
                const originalText = button.textContent;
                button.textContent = 'Copied!';
                button.style.backgroundColor = '#28a745';
                
                setTimeout(function() {
                    button.textContent = originalText;
                    button.style.backgroundColor = '';
                }, 2000);
            }).catch(function(err) {
                console.error('Failed to copy text: ', err);
                // Fallback for older browsers
                const textArea = document.createElement('textarea');
                textArea.value = text;
                document.body.appendChild(textArea);
                textArea.select();
                document.execCommand('copy');
                document.body.removeChild(textArea);
                
                const originalText = button.textContent;
                button.textContent = 'Copied!';
                button.style.backgroundColor = '#28a745';
                
                setTimeout(function() {
                    button.textContent = originalText;
                    button.style.backgroundColor = '';
                }, 2000);
            });
        }
    </script>
</body>
</html>